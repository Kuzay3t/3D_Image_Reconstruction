{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kuzay3t/3D_Image_Reconstruction/blob/main/3d_Reconstruction_ds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open3d"
      ],
      "metadata": {
        "id": "r_Jd49NRbZ4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894ad094-b358-4d1c-9dd1-84908856e66b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open3d in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.0.2)\n",
            "Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.11/dist-packages (from open3d) (1.7.1)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from open3d) (8.1.7)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (from open3d) (2.4.0)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.3.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.11/dist-packages (from open3d) (0.9.9)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.4.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.0.2)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.25.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.26.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.7.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "snZn2ldyAxpK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cgFTOVmXsC-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d84173-643d-4c4c-ac06-b67a994479bb"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "15Ipfnj48f8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230c647b-677c-4171-f4ef-81a174c0b95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['FINANCIAL LITERACY CERTIFICATE.pdf', 'KUZAYET BMC.pdf', 'GST 208 RECEIPT.pdf', 'VID-20240508-WA0014.mp4', 'BAGAI GLORY RESUME.pdf', 'BAGAI GLORY HEADSHOT.jpeg', '300LVL 2ND SEMESTER', 'project proposal template', 'RENUE DOCUMENT', 'Colab Notebooks', 'archive (1).zip', 'archive.zip']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir('/content/drive/My Drive'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Oh6fybBIFEqv"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/drive/My Drive/archive.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "tuz8xMBa9glU"
      },
      "outputs": [],
      "source": [
        "extract_path = '/content/ModelNet10'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191f9008"
      },
      "source": [
        "#Setting path to unzipped dataset\n",
        "root_dir = '/content/ModelNet10/ModelNet10'\n",
        "category = 'chair'\n",
        "model_dir = os.path.join(root_dir, category, 'train')\n",
        "sample_file = [f for f in os.listdir(model_dir) if f.endswith('.off')][0]\n",
        "file_path = os.path.join(model_dir, sample_file)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70f7c779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035d6d03-d8d2-4671-a22b-bad77c407e55"
      },
      "source": [
        "print(os.listdir('/content/ModelNet10'))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['metadata_modelnet10.csv', 'ModelNet10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "_GrQuxQzTdxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Listig the contents of the extracted ModelNet10 directory\n",
        "print(os.listdir(extract_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1nqLSPea_9c",
        "outputId": "dd0393d2-afc1-4755-df81-0e784dd62e47"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['metadata_modelnet10.csv', 'ModelNet10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ModelNet10 has two subdirectories: 'train' and 'test'\n",
        "train_path = os.path.join(extract_path, 'ModelNet10')\n",
        "test_path = os.path.join(extract_path, 'ModelNet10')"
      ],
      "metadata": {
        "id": "sRexppq3bDOn"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the categories available in the training set\n",
        "categories = os.listdir(train_path)\n",
        "print(\"Available categories:\", categories)"
      ],
      "metadata": {
        "id": "NJg2twBxaqrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d73ec8-9d77-41ed-fd6d-f66709ae16d1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available categories: ['desk', 'bathtub', 'toilet', 'sofa', 'table', 'bed', 'night_stand', 'chair', 'monitor', 'dresser']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and Visualizing the 3D Model**"
      ],
      "metadata": {
        "id": "hLylqEGca2D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_visualize_model(category='chair', sample_index=0):\n",
        "    \"\"\"\n",
        "    Load and visualize a 3D model from ModelNet10\n",
        "\n",
        "    Args:\n",
        "        category (str): Category of the model to load (e.g., 'chair', 'table')\n",
        "        sample_index (int): Index of the model to load within its category\n",
        "    \"\"\"\n",
        "    # Constructing the path to the sample model\n",
        "    category_train_path = os.path.join(train_path, category, 'train')\n",
        "    model_files = [f for f in os.listdir(category_train_path) if f.endswith('.off')]\n",
        "    model_file = os.path.join(category_train_path, model_files[sample_index])\n",
        "\n",
        "    # Loading the mesh file\n",
        "    mesh = o3d.io.read_triangle_mesh(model_file)\n",
        "\n",
        "    # Printing basic information about the mesh\n",
        "    print(f\"Loaded {category} model: {model_files[sample_index]}\") # Corrected variable name\n",
        "    print(\"Number of vertices:\", len(mesh.vertices))\n",
        "    print(\"Number of triangles:\", len(mesh.triangles))\n",
        "\n",
        "    # Computing vertex normals for better visualization\n",
        "    mesh.compute_vertex_normals()\n",
        "\n",
        "    # Visualizing the mesh\n",
        "    o3d.visualization.draw_geometries([mesh], window_name=f\"Original {category}\")"
      ],
      "metadata": {
        "id": "UOCjxT95azu6"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_visualize_model('chair', 0)"
      ],
      "metadata": {
        "id": "usnW8TLobNM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eafcee68-f4ad-4f23-dd41-c6dbbd2df40d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded chair model: chair_0396.off\n",
            "Number of vertices: 3644\n",
            "Number of triangles: 4088\n",
            "\u001b[1;33m[Open3D WARNING] GLFW Error: Failed to detect any supported platform\u001b[0;m\n",
            "\u001b[1;33m[Open3D WARNING] GLFW initialized for headless rendering.\u001b[0;m\n",
            "\u001b[1;33m[Open3D WARNING] GLFW Error: OSMesa: Library not found\u001b[0;m\n",
            "\u001b[1;33m[Open3D WARNING] Failed to create window\u001b[0;m\n",
            "\u001b[1;33m[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.\u001b[0;m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "ZBD2GWe1bWa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_model(mesh, num_points=1024):\n",
        "    \"\"\"\n",
        "    Convert mesh to point cloud and normalize it\n",
        "\n",
        "    Args:\n",
        "        mesh: Open3D mesh object\n",
        "        num_points: Number of points to sample from the mesh\n",
        "\n",
        "    Returns:\n",
        "        point_cloud: Normalized point cloud as numpy array\n",
        "    \"\"\"\n",
        "    # Converting mesh to point cloud by sampling points\n",
        "    pcd = mesh.sample_points_poisson_disk(number_of_points=num_points)\n",
        "\n",
        "    # Getting points as numpy array\n",
        "    points = np.asarray(pcd.points)\n",
        "\n",
        "    # Normalizing points to [-1, 1] range\n",
        "    centroid = np.mean(points, axis=0)\n",
        "    points -= centroid\n",
        "    max_dist = np.max(np.sqrt(np.sum(points**2, axis=1)))\n",
        "    points /= max_dist\n",
        "\n",
        "    return points\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQpzHxZ_bU0-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chair_files = [f for f in os.listdir(os.path.join(train_path, 'chair', 'train')) if f.endswith('.off')]\n",
        "if chair_files:\n",
        "    sample_chair_file = chair_files[0]\n",
        "    mesh = o3d.io.read_triangle_mesh(os.path.join(train_path, 'chair', 'train', sample_chair_file))\n",
        "\n",
        "    if mesh.has_triangles():\n",
        "        point_cloud = preprocess_model(mesh)\n",
        "        print(\"Normalized point cloud shape:\", point_cloud.shape)\n",
        "    else:\n",
        "        print(f\"Loaded mesh from {sample_chair_file} has no triangles and cannot be processed.\")\n",
        "else:\n",
        "    print(f\"No .off files found in {os.path.join(train_path, 'chair', 'train')}\")"
      ],
      "metadata": {
        "id": "OrV7TE0hb4DG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a67d93-c14f-40d3-ee48-bb18d90598de"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized point cloud shape: (1024, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Artificial Noise"
      ],
      "metadata": {
        "id": "B2AqsWdfcCZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise_to_point_cloud(points, noise_level=0.05):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise to a point cloud\n",
        "\n",
        "    Args:\n",
        "        points: Input point cloud (N x 3)\n",
        "        noise_level: Standard deviation of Gaussian noise\n",
        "\n",
        "    Returns:\n",
        "        noisy_points: Noisy version of input point cloud\n",
        "    \"\"\"\n",
        "    noise = np.random.normal(scale=noise_level, size=points.shape)\n",
        "    noisy_points = points + noise\n",
        "\n",
        "\n",
        "    noisy_points = np.clip(noisy_points, -1, 1)\n",
        "\n",
        "    return noisy_points\n",
        "\n"
      ],
      "metadata": {
        "id": "GzlMJjBIcBoB"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "noisy_pc = add_noise_to_point_cloud(point_cloud)\n",
        "print(\"Noisy point cloud shape:\", noisy_pc.shape)"
      ],
      "metadata": {
        "id": "W0AKk9EUb3-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b3e8b2-f2aa-40a6-dbd8-024d163426e6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noisy point cloud shape: (1024, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Denoising Model"
      ],
      "metadata": {
        "id": "jCUUgaiRcL6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNetDenoiser(nn.Module):\n",
        "    def __init__(self, num_points=1024):\n",
        "        super(PointNetDenoiser, self).__init__()\n",
        "\n",
        "        # Shared MLP for point features\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Conv1d(3, 64, 1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 64, 1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Second MLP for higher-level features\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, 1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 1024, 1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder MLP to get back to 3D coordinates\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv1d(1088, 512, 1),  # 1024 (global) + 64 (local) = 1088\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(512, 256, 1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 3, 1)  # Output 3D coordinates\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, 3, num_points)\n",
        "        batch_size = x.size(0)\n",
        "        num_points = x.size(2)\n",
        "\n",
        "        # Local feature extraction\n",
        "        local_features = self.mlp1(x)  # (batch_size, 64, num_points)\n",
        "\n",
        "        # Global feature extraction\n",
        "        global_features = self.mlp2(local_features)  # (batch_size, 1024, num_points)\n",
        "        global_features = torch.max(global_features, 2, keepdim=True)[0]  # (batch_size, 1024, 1)\n",
        "        global_features = global_features.repeat(1, 1, num_points)  # (batch_size, 1024, num_points)\n",
        "\n",
        "        # Concatenate local and global features\n",
        "        combined_features = torch.cat([local_features, global_features], 1)  # (batch_size, 1088, num_points)\n",
        "\n",
        "        # Decode to denoised points\n",
        "\n",
        "        return self.decoder(combined_features)"
      ],
      "metadata": {
        "id": "5117NGMHb34R"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Denoising Model"
      ],
      "metadata": {
        "id": "xRgrTzM7cY5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_denoising_model(train_data, num_epochs=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train the denoising model\n",
        "\n",
        "    Args:\n",
        "        train_data: List of preprocessed point clouds\n",
        "        num_epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "    \"\"\"\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "    optimizer = optim.Adam(PointNetDenoiser().parameters(), lr=0.001)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    # First create pairs of (noisy, clean) point clouds\n",
        "    noisy_train = [add_noise_to_point_cloud(pc) for pc in train_data]\n",
        "    train_data = [torch.tensor(pc, dtype=torch.float32).transpose(0, 1) for pc in train_data]  # (3, num_points)\n",
        "    noisy_train = [torch.tensor(pc, dtype=torch.float32).transpose(0, 1) for pc in noisy_train]\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        # Mini-batch training\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            batch_clean = torch.stack(train_data[i:i+batch_size])\n",
        "            batch_noisy = torch.stack(noisy_train[i:i+batch_size])\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = PointNetDenoiser()(batch_noisy)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, batch_clean)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_data):.4f}\")\n",
        "\n",
        "    return PointNetDenoiser()"
      ],
      "metadata": {
        "id": "3e-iix_eb30D"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Results"
      ],
      "metadata": {
        "id": "pJd21tCXclFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_denoising_results(model, test_data, num_samples=3):\n",
        "    \"\"\"\n",
        "    Visualize original, noisy, and denoised point clouds\n",
        "\n",
        "    Args:\n",
        "        model: Trained denoising model\n",
        "        test_data: List of clean test point clouds\n",
        "        num_samples: Number of samples to visualize\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Get a test sample\n",
        "        clean_pc = test_data[i]\n",
        "        noisy_pc = add_noise_to_point_cloud(clean_pc)\n",
        "\n",
        "        # Convert to tensor format the model expects\n",
        "        noisy_tensor = torch.tensor(noisy_pc, dtype=torch.float32).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "        # Denoise with our model\n",
        "        with torch.no_grad():\n",
        "            denoised_tensor = model(noisy_tensor)\n",
        "\n",
        "        # Convert back to numpy\n",
        "        denoised_pc = denoised_tensor.squeeze(0).transpose(0, 1).numpy()\n",
        "\n",
        "        # Create Open3D point cloud objects for visualization\n",
        "        clean_pcd = o3d.geometry.PointCloud()\n",
        "        clean_pcd.points = o3d.utility.Vector3dVector(clean_pc)\n",
        "        clean_pcd.paint_uniform_color([0, 1, 0])  # Green for clean\n",
        "\n",
        "        noisy_pcd = o3d.geometry.PointCloud()\n",
        "        noisy_pcd.points = o3d.utility.Vector3dVector(noisy_pc)\n",
        "        noisy_pcd.paint_uniform_color([1, 0, 0])  # Red for noisy\n",
        "\n",
        "        denoised_pcd = o3d.geometry.PointCloud()\n",
        "        denoised_pcd.points = o3d.utility.Vector3dVector(denoised_pc)\n",
        "        denoised_pcd.paint_uniform_color([0, 0, 1])  # Blue for denoised\n",
        "\n",
        "        # Visualize together\n",
        "        o3d.visualization.draw_geometries(\n",
        "            [clean_pcd, noisy_pcd, denoised_pcd],\n",
        "            window_name=f\"Sample {i+1}: Green=Clean, Red=Noisy, Blue=Denoised\"\n",
        "        )"
      ],
      "metadata": {
        "id": "myW_lv_Xb3xd"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Results"
      ],
      "metadata": {
        "id": "5OYEc6IhgenJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import KDTree\n",
        "\n",
        "def chamfer_distance(pc1, pc2):\n",
        "    \"\"\"\n",
        "    Compute Chamfer distance between two point clouds\n",
        "\n",
        "    Args:\n",
        "        pc1: First point cloud (N x 3)\n",
        "        pc2: Second point cloud (M x 3)\n",
        "\n",
        "    Returns:\n",
        "        chamfer_dist: Chamfer distance between pc1 and pc2\n",
        "    \"\"\"\n",
        "    # Build KDTree for efficient nearest neighbor search\n",
        "    tree1 = KDTree(pc1)\n",
        "    tree2 = KDTree(pc2)\n",
        "\n",
        "    # For each point in pc2, find nearest in pc1\n",
        "    dist1, _ = tree1.query(pc2)\n",
        "    # For each point in pc1, find nearest in pc2\n",
        "    dist2, _ = tree2.query(pc1)\n",
        "\n",
        "    # Chamfer distance is the sum of average distances\n",
        "    chamfer_dist = np.mean(dist1) + np.mean(dist2)\n",
        "    return chamfer_dist\n"
      ],
      "metadata": {
        "id": "E0By7eeggw0U"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def hausdorff_distance(pc1, pc2):\n",
        "    \"\"\"\n",
        "    Compute Hausdorff distance between two point clouds\n",
        "\n",
        "    Args:\n",
        "        pc1: First point cloud (N x 3)\n",
        "        pc2: Second point cloud (M x 3)\n",
        "\n",
        "    Returns:\n",
        "        hausdorff_dist: Hausdorff distance between pc1 and pc2\n",
        "    \"\"\"\n",
        "    tree1 = KDTree(pc1)\n",
        "    tree2 = KDTree(pc2)\n",
        "\n",
        "    # Find nearest neighbors both ways\n",
        "    dist1, _ = tree1.query(pc2)\n",
        "    dist2, _ = tree2.query(pc1)\n",
        "\n",
        "    # Hausdorff distance is the maximum of all minimum distances\n",
        "    hausdorff_dist = max(np.max(dist1), np.max(dist2))\n",
        "    return hausdorff_dist"
      ],
      "metadata": {
        "id": "g5he3OHlg26t"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def point_to_point_distance(pc1, pc2):\n",
        "    \"\"\"\n",
        "    Compute average point-to-point distance (for aligned point clouds)\n",
        "\n",
        "    Args:\n",
        "        pc1: First point cloud (N x 3)\n",
        "        pc2: Second point cloud (N x 3)\n",
        "\n",
        "    Returns:\n",
        "        avg_distance: Average Euclidean distance between corresponding points\n",
        "    \"\"\"\n",
        "    # Ensure point clouds have same number of points\n",
        "    assert pc1.shape == pc2.shape\n",
        "    distances = np.linalg.norm(pc1 - pc2, axis=1)\n",
        "    return np.mean(distances)"
      ],
      "metadata": {
        "id": "wmLsWlrkg79U"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_denoising_model(model, test_data, num_samples=5, visualize=True):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the denoising model\n",
        "\n",
        "    Args:\n",
        "        model: Trained denoising model\n",
        "        test_data: List of clean test point clouds\n",
        "        num_samples: Number of samples to evaluate\n",
        "        visualize: Whether to show visual results\n",
        "\n",
        "    Returns:\n",
        "        metrics_dict: Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Initialize metric accumulators\n",
        "    chamfer_clean_noisy = []\n",
        "    chamfer_clean_denoised = []\n",
        "    hausdorff_clean_noisy = []\n",
        "    hausdorff_clean_denoised = []\n",
        "    ptp_clean_noisy = []\n",
        "    ptp_clean_denoised = []\n",
        "\n",
        "    # Select random samples if we have more than num_samples\n",
        "    if len(test_data) > num_samples:\n",
        "        indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
        "        test_samples = [test_data[i] for i in indices]\n",
        "    else:\n",
        "        test_samples = test_data\n",
        "\n",
        "    for i, clean_pc in enumerate(test_samples):\n",
        "        # Generate noisy version\n",
        "        noisy_pc = add_noise_to_point_cloud(clean_pc)\n",
        "\n",
        "        # Convert to tensor format the model expects\n",
        "        noisy_tensor = torch.tensor(noisy_pc, dtype=torch.float32).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "        # Denoise with our model\n",
        "        with torch.no_grad():\n",
        "            denoised_tensor = model(noisy_tensor)\n",
        "\n",
        "        # Convert back to numpy\n",
        "        denoised_pc = denoised_tensor.squeeze(0).transpose(0, 1).numpy()\n",
        "\n",
        "        # Compute metrics\n",
        "        cd_noisy = chamfer_distance(clean_pc, noisy_pc)\n",
        "        cd_denoised = chamfer_distance(clean_pc, denoised_pc)\n",
        "        hd_noisy = hausdorff_distance(clean_pc, noisy_pc)\n",
        "        hd_denoised = hausdorff_distance(clean_pc, denoised_pc)\n",
        "        ptp_noisy = point_to_point_distance(clean_pc, noisy_pc)\n",
        "        ptp_denoised = point_to_point_distance(clean_pc, denoised_pc)\n",
        "\n",
        "        # Store metrics\n",
        "        chamfer_clean_noisy.append(cd_noisy)\n",
        "        chamfer_clean_denoised.append(cd_denoised)\n",
        "        hausdorff_clean_noisy.append(hd_noisy)\n",
        "        hausdorff_clean_denoised.append(hd_denoised)\n",
        "        ptp_clean_noisy.append(ptp_noisy)\n",
        "        ptp_clean_denoised.append(ptp_denoised)\n",
        "\n",
        "        # Visualization\n",
        "        if visualize:\n",
        "            # Create Open3D point cloud objects\n",
        "            clean_pcd = o3d.geometry.PointCloud()\n",
        "            clean_pcd.points = o3d.utility.Vector3dVector(clean_pc)\n",
        "            clean_pcd.paint_uniform_color([0, 1, 0])  # Green for clean\n",
        "\n",
        "            noisy_pcd = o3d.geometry.PointCloud()\n",
        "            noisy_pcd.points = o3d.utility.Vector3dVector(noisy_pc)\n",
        "            noisy_pcd.paint_uniform_color([1, 0, 0])  # Red for noisy\n",
        "\n",
        "            denoised_pcd = o3d.geometry.PointCloud()\n",
        "            denoised_pcd.points = o3d.utility.Vector3dVector(denoised_pc)\n",
        "            denoised_pcd.paint_uniform_color([0, 0, 1])  # Blue for denoised\n",
        "\n",
        "            # Visualize together\n",
        "            o3d.visualization.draw_geometries(\n",
        "                [clean_pcd, noisy_pcd, denoised_pcd],\n",
        "                window_name=f\"Sample {i+1} - CD: Noisy={cd_noisy:.4f}, Denoised={cd_denoised:.4f}\"\n",
        "            )\n",
        "\n",
        "            # Print metrics for this sample\n",
        "            print(f\"\\nSample {i+1} Metrics:\")\n",
        "            print(f\"Chamfer Distance: Noisy={cd_noisy:.4f}, Denoised={cd_denoised:.4f}\")\n",
        "            print(f\"Hausdorff Distance: Noisy={hd_noisy:.4f}, Denoised={hd_denoised:.4f}\")\n",
        "            print(f\"Point-to-Point: Noisy={ptp_noisy:.4f}, Denoised={ptp_denoised:.4f}\")\n",
        "\n",
        "    # Compute average metrics across all samples\n",
        "    metrics_dict = {\n",
        "        'avg_chamfer_noisy': np.mean(chamfer_clean_noisy),\n",
        "        'avg_chamfer_denoised': np.mean(chamfer_clean_denoised),\n",
        "        'avg_hausdorff_noisy': np.mean(hausdorff_clean_noisy),\n",
        "        'avg_hausdorff_denoised': np.mean(hausdorff_clean_denoised),\n",
        "        'avg_ptp_noisy': np.mean(ptp_clean_noisy),\n",
        "        'avg_ptp_denoised': np.mean(ptp_clean_denoised),\n",
        "        'chamfer_improvement': np.mean(chamfer_clean_noisy) - np.mean(chamfer_clean_denoised),\n",
        "        'hausdorff_improvement': np.mean(hausdorff_clean_noisy) - np.mean(hausdorff_clean_denoised),\n",
        "        'ptp_improvement': np.mean(ptp_clean_noisy) - np.mean(ptp_clean_denoised)\n",
        "    }\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\n=== Summary Evaluation Metrics ===\")\n",
        "    print(f\"Average Chamfer Distance:\")\n",
        "    print(f\"  Noisy: {metrics_dict['avg_chamfer_noisy']:.4f}\")\n",
        "    print(f\"  Denoised: {metrics_dict['avg_chamfer_denoised']:.4f}\")\n",
        "    print(f\"  Improvement: {metrics_dict['chamfer_improvement']:.4f} ({(metrics_dict['chamfer_improvement']/metrics_dict['avg_chamfer_noisy'])*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nAverage Hausdorff Distance:\")\n",
        "    print(f\"  Noisy: {metrics_dict['avg_hausdorff_noisy']:.4f}\")\n",
        "    print(f\"  Denoised: {metrics_dict['avg_hausdorff_denoised']:.4f}\")\n",
        "    print(f\"  Improvement: {metrics_dict['hausdorff_improvement']:.4f} ({(metrics_dict['hausdorff_improvement']/metrics_dict['avg_hausdorff_noisy'])*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nAverage Point-to-Point Distance:\")\n",
        "    print(f\"  Noisy: {metrics_dict['avg_ptp_noisy']:.4f}\")\n",
        "    print(f\"  Denoised: {metrics_dict['avg_ptp_denoised']:.4f}\")\n",
        "    print(f\"  Improvement: {metrics_dict['ptp_improvement']:.4f} ({(metrics_dict['ptp_improvement']/metrics_dict['avg_ptp_noisy'])*100:.1f}%)\")\n",
        "\n",
        "    return metrics_dict"
      ],
      "metadata": {
        "id": "bhhTAExfhBmM"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluate_denoising_model(model, test_data, num_samples=5)\n",
        "\n",
        "\n",
        "print(\"\\nKey Metrics:\")\n",
        "print(f\"Chamfer Improvement: {metrics['chamfer_improvement']:.4f}\")\n",
        "print(f\"Hausdorff Improvement: {metrics['hausdorff_improvement']:.4f}\")\n",
        "print(f\"Percentage Improvement: {metrics['chamfer_improvement']/metrics['avg_chamfer_noisy']*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4FoHkFc1hOIw",
        "outputId": "1630f178-219f-4e68-de3d-8daddb034583"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-110-3160904287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_denoising_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nKey Metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Chamfer Improvement: {metrics['chamfer_improvement']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics_comparison(metrics_dict):\n",
        "    \"\"\"\n",
        "    Plot a bar chart comparing noisy vs denoised metrics\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    labels = ['Chamfer', 'Hausdorff', 'Point-to-Point']\n",
        "    noisy_vals = [\n",
        "        metrics_dict['avg_chamfer_noisy'],\n",
        "        metrics_dict['avg_hausdorff_noisy'],\n",
        "        metrics_dict['avg_ptp_noisy']\n",
        "    ]\n",
        "    denoised_vals = [\n",
        "        metrics_dict['avg_chamfer_denoised'],\n",
        "        metrics_dict['avg_hausdorff_denoised'],\n",
        "        metrics_dict['avg_ptp_denoised']\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    rects1 = ax.bar(x - width/2, noisy_vals, width, label='Noisy', color='red')\n",
        "    rects2 = ax.bar(x + width/2, denoised_vals, width, label='Denoised', color='blue')\n",
        "\n",
        "    ax.set_ylabel('Distance Value')\n",
        "    ax.set_title('Comparison of Noisy vs Denoised Metrics')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_visual_comparison(model, test_data, save_dir='results'):\n",
        "    \"\"\"\n",
        "    Save visual comparisons of original, noisy and denoised point clouds\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, clean_pc in enumerate(test_data[:5]):  # Save first 5 samples\n",
        "        noisy_pc = add_noise_to_point_cloud(clean_pc)\n",
        "        noisy_tensor = torch.tensor(noisy_pc, dtype=torch.float32).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            denoised_tensor = model(noisy_tensor)\n",
        "\n",
        "        denoised_pc = denoised_tensor.squeeze(0).transpose(0, 1).numpy()\n",
        "\n",
        "        # Create and save individual point clouds\n",
        "        for name, pc, color in zip(\n",
        "            ['clean', 'noisy', 'denoised'],\n",
        "            [clean_pc, noisy_pc, denoised_pc],\n",
        "            [[0,1,0], [1,0,0], [0,0,1]]\n",
        "        ):\n",
        "            pcd = o3d.geometry.PointCloud()\n",
        "            pcd.points = o3d.utility.Vector3dVector(pc)\n",
        "            pcd.paint_uniform_color(color)\n",
        "            o3d.io.write_point_cloud(f\"{save_dir}/sample_{i}_{name}.ply\", pcd)\n",
        "\n",
        "        print(f\"Saved sample {i} to {save_dir}\")"
      ],
      "metadata": {
        "id": "jqP1XBnFhnab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load your test data (assuming you have this from earlier)\n",
        "test_data = load_dataset_samples('chair', 10)  # Load 10 chair models for testing\n",
        "\n",
        "# 2. Run the evaluation\n",
        "metrics = evaluate_denoising_model(model, test_data, num_samples=5, visualize=True)\n",
        "\n",
        "# 3. Plot metrics comparison\n",
        "plot_metrics_comparison(metrics)\n",
        "\n",
        "# 4. Save visual results\n",
        "save_visual_comparison(model, test_data)\n",
        "\n",
        "# 5. Print final evaluation summary\n",
        "print(\"\\n=== Final Evaluation Summary ===\")\n",
        "print(\"The model shows the following improvements:\")\n",
        "print(f\"- Chamfer Distance: {metrics['chamfer_improvement']:.4f} improvement\")\n",
        "print(f\"- Hausdorff Distance: {metrics['hausdorff_improvement']:.4f} improvement\")\n",
        "print(f\"- Point-to-Point Distance: {metrics['ptp_improvement']:.4f} improvement\")"
      ],
      "metadata": {
        "id": "UaR0dscChbDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/taQYGgWkP7w4bh3J9NF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}